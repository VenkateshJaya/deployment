{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51c95d1-b447-4f1b-9571-cf597ca93ef4",
   "metadata": {},
   "source": [
    "<span style=\"display: block;  text-align: center; color:#8735fb; font-size:22pt\"> **HPO Benchmarking with RAPIDS and Dask** </span>\n",
    "\n",
    "Hyper-Parameter Optimization (HPO) helps to find the best version of a model by exploring the space of possible configurations. While generally desirable, this search is computationally expensive and time-consuming.\n",
    "\n",
    "In the notebook demo below, we compare benchmarking results to show how GPU can accelerate HPO tuning jobs relative to CPU.\n",
    "\n",
    "For instance, we find a x speedup in wall clock time and a x reduction in cost when comparing between GPU and CPU EC2 instances on 100 XGBoost HPO trials on 3 years of the Airline Dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d178f5-fc5d-471e-9898-544e5fdbc271",
   "metadata": {},
   "source": [
    "<span style=\"display: block;  color:#8735fb; font-size:22pt\"> **Preamble** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c311bd4-76a1-4ee7-8841-5d44ca052566",
   "metadata": {},
   "source": [
    "You can set up local environment but it is recommended to launch a Virtual Machine service (Azure, AWS, GCP, etc).\n",
    "\n",
    "For the purposes of this notebook, we will be utilizing the [Amazon Machine Image (AMI)](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-gpu-tensorflow-2-12-amazon-linux-2/) as the starting point.\n",
    "\n",
    "\n",
    "````{docref} /cloud/aws/\n",
    "Please follow instructions in [AWS Elastic Cloud Compute)](../../cloud/aws/ec2) to launch an EC2 instance with GPUs, the NVIDIA Driver and the NVIDIA Container Runtime.\n",
    "\n",
    "```{note}\n",
    "When configuring your instance ensure you select the [Deep Learning AMI GPU TensorFlow or PyTorch](https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html) in the AMI selection box under **\"Amazon Machine Image (AMI)\"**\n",
    "\n",
    "![](../../_static/images/examples/xgboost-rf-gpu-cpu-benchmark/amazon-deeplearning-ami.png)\n",
    "```\n",
    "\n",
    "Once your instance is running and you have access to Jupyter save this notebook and run through the cells.\n",
    "\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d57ba2-df8a-4757-a0df-44b3cd73b75c",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **ML Workflow** </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3fbaf-754b-45a6-959c-7163edfe5c4f",
   "metadata": {},
   "source": [
    "<span style=\"display: block; font-size:20pt\"> Dataset </span>\n",
    "\n",
    "We leverage the `Airline` dataset, which is a large public tracker of US domestic flight logs which we offer in various sizes (1 year, 3 year, and 10 year) and in [Parquet](https://parquet.apache.org/) (compressed column storage) format. The machine learning objective with this dataset is to predict whether flights will be more than 15 minutes late arriving to their destination.\n",
    "\n",
    "We host the demo dataset in public S3 demo buckets in both the `us-east-1` or `us-west-2`. To optimize performance, we recommend that you access the s3 bucket in the same region as your EC2 instance to reduce network latency and data transfer costs. \n",
    "\n",
    "For this demo, we are using the 3_year dataset, which includes the following features to mention a few:\n",
    "\n",
    "* Locations and distance ( Origin, Dest, Distance )\n",
    "* Airline / carrier ( Reporting_Airline )\n",
    "* Scheduled departure and arrival times ( CRSDepTime and CRSArrTime )\n",
    "* Actual departure and arrival times ( DpTime and ArrTime )\n",
    "* Difference between scheduled & actual times ( ArrDelay and DepDelay )\n",
    "* Binary encoded version of late, aka our target variable ( ArrDelay15 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b5a5b-87cf-4ff6-84f2-4485c7c0470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure aws credentials for access to S3 storage\n",
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd10f79-23b2-440e-89b4-f6e4a5c9f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset from S3 bucket to your current working dir\n",
    "!aws s3 cp --recursive s3://sagemaker-rapids-hpo-us-west-2/3_year/ ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc101b-3206-4f86-9b0d-d4313ad1ee43",
   "metadata": {},
   "source": [
    "<span style=\"display: block; font-size:20pt\"> Algorithm </span>\n",
    "\n",
    "From a ML/algorithm perspective, we offer `XGBoost` and `RandomForest`. You are free to switch between these algorithm choices and everything in the example will continue to work.\n",
    "\n",
    "We can also optionally increase robustness via reshuffles of the train-test split (i.e., cross-validation folds). Typical values here are between 3 and 10 folds but we will use `n_cv_folds = 5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef53b4-d7e0-43e2-b85b-372bd2d882f7",
   "metadata": {},
   "source": [
    "<span style=\"display: block; font-size:20pt\"> Search Range </span>\n",
    "\n",
    "In order to work with RAPIDS container, the entrypoint logic should parse arguments, load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyperparameter setting.\n",
    "\n",
    "`Optuna` is a hyperparameter optimization library in Python. We create an Optuna study object that provides a framework to define the search space, objective function, and optimization algorithm for the hpo process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac0c4b-5581-4bd9-ae4e-e4aedef30ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b1759de-98af-4628-a79b-a236a2dee5a2",
   "metadata": {},
   "source": [
    "<span style=\"display: block; font-size:20pt\"> Local Cluster </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533be0b1-0d5e-46b3-9ff1-dd71751fe68f",
   "metadata": {},
   "source": [
    "To maximize on efficiency, we launch either a `LocalCluster` for cpu or `LocalCUDACluster` that utilizes GPUs for distributed computing. Then connect a Dask Client to submit and manage computations on the cluster. \n",
    "\n",
    "WE can then submit and \"persist\" the dataset to the Dask client, instructing Dask to store the dataset in memory  at all times for faster performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89edfea-ca14-4d26-94c6-0ef8eaf02d77",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **Build RAPIDS Container** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919b0eb-70f6-43cb-8d64-5bc9e3b2ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07ad98-9d9d-4f9f-ba4e-1b0a1646cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72cbf8-1f93-4e63-a767-d87210697eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0ccfd-dc68-4c1a-8a31-e74a5d02b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t rapids-tco-benchmark:v23.06 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1959fa-f70d-4940-8204-1946acd0e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b8b7f-1634-42b8-81e8-2605195d2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tmux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b59cf6-963e-4a08-85f7-267b83a99cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --gpus all -p 8888:8888 -p 8787:8787 -p 8786:8786 -v \\\n",
    "                    /home/ec2-user/tco_hpo_gpu_cpu_perf_benchmark:/rapids/notebooks/host \\\n",
    "                            rapids-tco-benchmark:v23.06 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f881ee-ce7f-4810-a6b7-fa8aa72d91f3",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **Run HPO** </span>\n",
    "\n",
    "Navigate to the host directory inside the container and run the python script with the following command : \n",
    "\n",
    "    python ./hpo.py --model-type \"XGBoost\" --mode \"gpu\"  > xgboost_gpu.txt 2>&1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.06",
   "language": "python",
   "name": "rapids-23.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
