{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51c95d1-b447-4f1b-9571-cf597ca93ef4",
   "metadata": {},
   "source": [
    "<span style=\"display: block;  text-align: center; color:#8735fb; font-size:22pt\"> **HPO Benchmarking with RAPIDS and Dask** </span>\n",
    "\n",
    "Hyper-Parameter Optimization (HPO) helps to find the best version of a model by exploring the space of possible configurations. While generally desirable, this search is computationally expensive and time-consuming.\n",
    "\n",
    "In the notebook demo below, we compare benchmarking results to show how GPU can accelerate HPO tuning jobs relative to CPU.\n",
    "\n",
    "For instance, we find a x speedup in wall clock time (6 hours vs 3+ days) and a x reduction in cost when comparing between GPU and CPU EC2 instances on 100 XGBoost HPO trials using 10 parallel workers on 10 years of the Airline Dataset.\n",
    "\n",
    "For more check out our AWS blog(link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d178f5-fc5d-471e-9898-544e5fdbc271",
   "metadata": {},
   "source": [
    "<span style=\"display: block;  color:#8735fb; font-size:22pt\"> **Preamble** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c311bd4-76a1-4ee7-8841-5d44ca052566",
   "metadata": {},
   "source": [
    "You can set up local environment but it is recommended to launch a Virtual Machine service (Azure, AWS, GCP, etc).\n",
    "\n",
    "For the purposes of this notebook, we will be utilizing the [Amazon Machine Image (AMI)](https://aws.amazon.com/releasenotes/aws-deep-learning-ami-gpu-tensorflow-2-12-amazon-linux-2/) as the starting point.\n",
    "\n",
    "\n",
    "````{docref} /cloud/aws/\n",
    "Please follow instructions in [AWS Elastic Cloud Compute)](../../cloud/aws/ec2) to launch an EC2 instance with GPUs, the NVIDIA Driver and the NVIDIA Container Runtime.\n",
    "\n",
    "```{note}\n",
    "When configuring your instance ensure you select the [Deep Learning AMI GPU TensorFlow or PyTorch](https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html) in the AMI selection box under **\"Amazon Machine Image (AMI)\"**\n",
    "\n",
    "![](../../_static/images/examples/xgboost-rf-gpu-cpu-benchmark/amazon-deeplearning-ami.png)\n",
    "```\n",
    "\n",
    "Once your instance is running and you have access to Jupyter save this notebook and run through the cells.\n",
    "\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3fbaf-754b-45a6-959c-7163edfe5c4f",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 2.1 - Dataset </span>\n",
    "\n",
    "The data source for this workflow is 3 years of the [Airline On-Time Statistics](https://www.transtats.bts.gov/ONTIME/) dataset from the US Bureau of Transportation.\n",
    "\n",
    "The public dataset contains logs/features about flights in the United States (17 airlines) including:\n",
    "\n",
    "* Locations and distance ( Origin, Dest, Distance )\n",
    "* Airline / carrier ( Reporting_Airline )\n",
    "* Scheduled departure and arrival times ( CRSDepTime and CRSArrTime )\n",
    "* Actual departure and arrival times ( DpTime and ArrTime )\n",
    "* Difference between scheduled & actual times ( ArrDelay and DepDelay )\n",
    "* Binary encoded version of late, aka our target variable ( ArrDelay15 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b5a5b-87cf-4ff6-84f2-4485c7c0470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd10f79-23b2-440e-89b4-f6e4a5c9f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DOWNLOAD THE DATASET\n",
    "!aws s3 cp --recursive s3://sagemaker-rapids-hpo-us-west-2/3_year/ ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1759de-98af-4628-a79b-a236a2dee5a2",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:20pt\"> 2.2 - Local Cluster </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533be0b1-0d5e-46b3-9ff1-dd71751fe68f",
   "metadata": {},
   "source": [
    "To maximize on efficiency, we launch a `LocalCUDACluster` that utilizes GPUs for distributed computing. Then connect a Dask Client to submit and manage computations on the cluster. Refer to this (link) for more information on how to achieve this.\n",
    "\n",
    "Submit dataset to the Dask client, instructing Dask to store the dataset in memory  at all times. This can improve performance by avoiding unnecessary data transfers during the hpo process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d57ba2-df8a-4757-a0df-44b3cd73b75c",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **ML Workflow** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddb8fd-2ac8-4257-8f7f-47f71c6d76c2",
   "metadata": {},
   "source": [
    "In order to work with RAPIDS container, the entrypoint logic should parse arguments, load and split data, build and train a model, score/evaluate the trained model, and emit an output representing the final score for the given hyperparameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcb2c4-526c-466e-88b6-00c202f494c2",
   "metadata": {},
   "source": [
    "`Optuna` is a hyperparameter optimization library in Python. We create an Optuna `study object` that provides a framework to define the search space, objective function, and optimization algorith for the hpo  process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d4ec4-b6e4-4546-8177-505f2739c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f715efe-0e23-4b12-aac3-49e0e3b575b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89edfea-ca14-4d26-94c6-0ef8eaf02d77",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **Build RAPIDS Container** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919b0eb-70f6-43cb-8d64-5bc9e3b2ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07ad98-9d9d-4f9f-ba4e-1b0a1646cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72cbf8-1f93-4e63-a767-d87210697eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0ccfd-dc68-4c1a-8a31-e74a5d02b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t rapids-tco-benchmark:v23.06 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1959fa-f70d-4940-8204-1946acd0e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b8b7f-1634-42b8-81e8-2605195d2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tmux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b59cf6-963e-4a08-85f7-267b83a99cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --gpus all -p 8888:8888 -p 8787:8787 -p 8786:8786 -v \\\n",
    "                    /home/ec2-user/tco_hpo_gpu_cpu_perf_benchmark:/rapids/notebooks/host \\\n",
    "                            rapids-tco-benchmark:v23.06 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f881ee-ce7f-4810-a6b7-fa8aa72d91f3",
   "metadata": {},
   "source": [
    "<span style=\"display: block; color:#8735fb; font-size:22pt\"> **Run HPO** </span>\n",
    "\n",
    "Navigate to the host directory inside the container and run the python script with the following command : \n",
    "\n",
    "    python ./hpo.py --model-type \"XGBoost\" --mode \"gpu\"  > xgboost_gpu.txt 2>&1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.06",
   "language": "python",
   "name": "rapids-23.06"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
